---
permalink: /
title: "ðŸ‘‹ About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a first-year master student at Shenzhen International Graduate School, Tsinghua University. I am fortunate to be supervised by Prof. [Yansong Tang](https://andytang15.github.io/) in IVG@SZ group. Before that, I got B.S. in Electric and Electronic Engineering from the University of Electronic and Science Technology of China (UESTC) in 2024.


My research interests lie in Computer Vision, such as Multimodal Learning,Segmentation, Tracking.

[Email](yuji-wan24@mails.tsinghua.edu.cn) / [Google Schoolar](https://scholar.google.com/citations?hl=en&user=oRpCyGkAAAAJ)

---
# âœ¨ News
---
* <span style="font-size: smaller;">2025-02: One paper on Triple Modalality Referring Segmentation is accepted to [CVPR 2025](https://cvpr.thecvf.com/)</span>
* <span style="font-size: smaller;">2024-12: One paper on Referring Image Segmentation is accepted to [AAAI 2025](https://aaai.org/conference/aaai/aaai-25/)</span>
* <span style="font-size: smaller;">2024-07: One paper on Multimodal Learning is accepted to [ECCV 2024](https://eccv.ecva.net/)</span>
<!-- * <span style="font-size: smaller;">2023-03: One paper on video understanding (Action Quality Assessment) is accepted to [CVPR 2023](https://cvpr.thecvf.com/Conferences/2023)</span> -->

---
# ðŸ”¬ Research
---
* indicates equal contribution
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

  <!--SAM2LOVE-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/sam2love.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle> 	SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes</papertitle>
      <br>
      <b>Yuji Wang*</b>, Haoran Xu*, Yong Liu, Jiaze Li, Yansong Tang 
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
      <!-- <br>
      <a href="https://arxiv.org/abs/2505.03730">[PDF]</a>
      <a href="https://github.com/shiyi-zh0408/FlexiAct">[Project Page]</a> 
      <br> -->
      <p> We propose a novel framework called SAM2-LOVE to effectively segment the video objects referred by the audio and text and achieve significant improvement in Ref-AVS tasks.</p>
    </td>
  </tr>	

  <!--IterPRime-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/iterprime.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>KV-Edit: Training-Free Image Editing for Precise Background Preservation</papertitle>
      <br>
      <b>Yuji Wang*</b>, Jingchen Ni*, Yong Liu, Chun Yuan, Yansong Tang 
      <br>
      AAAI Conference on Artificial Intelligence (<strong>AAAI<strong>),2025
      <br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32880">[PDF]</a>
      <a href="https://github.com/VoyageWang/IteRPrimE">[Project Page]</a> 
      <br>
      <p> We propose the novel IteRPrimE network to leverage the Grad-CAM for zero-shot referring image segmentation, which addresses the previous CLIP-based methodsâ€™ low robustness of positional phrases.</p>
    </td>
  </tr>	

  <!--DMRNET-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/dmrnet.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</papertitle>
      <br>
      Shicai Wei, Yang Luo, <b>Yuji Wang<b>, Chunbo Luo
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
      <br>
      <a href="https://arxiv.org/pdf/2407.04458">[PDF]</a>
      <a href="https://github.com/shicaiwei123/ECCV2024-DMRNet">[Project Page]</a> 
      <br>
      <p> We propose DMRNet improves multimodal learning with missing modalities by modeling inputs as probabilistic distributions to capture modality-specific information, outperforming state-of-the-art methods.</p>
    </td>
  </tr>	

  <!--LOGO-->
  <!-- <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/logo.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</papertitle>
      <br>
      <b>Shiyi Zhang</b>, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[PDF]</a>
      <a href="https://github.com/shiyi-zh0408/LOGO">[Project Page]</a> 
      <br>
      <p> LOGO is a new multi-person long-form video dataset for action quality assessment.</p>
    </td>
  </tr>	 -->

  <!--ManiGaussian-->
  <!-- <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/maga.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</papertitle>
      <br>
      Guanxing Lu, <b>Shiyi Zhang</b>, Ziwei Wang, Changliu Liu, Jiwen Lu and Yansong Tang.
      <br>
      <em>European Conference on Computer Vision (<strong>ECCV</strong>), 2024</em>
      <br>
      <a href="https://arxiv.org/pdf/2403.08321.pdf">[PDF]</a>
      <a href="https://github.com/GuanxingLu/ManiGaussian">[Project Page]</a> 
      <br>
      <p> We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.</p>
    </td>
  </tr>	 -->


</tbody></table>



